\chapter{Problem identification}
The chatbot (Tay) in itself did not malfunction on a technical level. The software did not contain flaws that resulted in a technical failure (e.g. a disability ability to post new tweets). On the contrary, it was not Microsoft’s intention to make the bot post tweets to could be categorized as offensive. The problem therefore has a much larger ethical aspect which has to be considered.
A question that arises is, could Microsoft have prevented this behavior by, for example, applying a filter to Tay’s communication layer? Although Microsoft states that they “implemented a lot of filtering and conducted extensive user studies with diverse user groups.” (Peter Lee, 2016) it remains unknown to what extend these tests were executed. In addition, if the conducted user studies really where extensive the vulnerability could possibly have been discovered at an earlier stage. The possibility arises that the developers knew about the vulnerability prior to the release of Tay.

\newpage

\section{Problem statement/ Objective}
The \textbf{problem statement}: “After 24 hours, Tay was taken offline for expressing anti-Semitic, racist and discriminating thoughts.”
The \textbf{objective} of this rapport: “Identify and research the technical as well as the ethical factors that could have led to the unexpected behaviour of Tay.”

\subsection{Research questions}
\textbf{Main question}: How could Tay’s algorithms have been abused after the extensive testing of Microsoft corp.?

\textbf{Sub-questions}:
\begin{itemize}
	\item How can frequency analysis algorithms be abused?
	\item What factors could possibly have influenced Tay to behave in such an unethical manner?
	\item Did the developers knew about the vulnerability prior to the release of Tay?
\end{itemize}